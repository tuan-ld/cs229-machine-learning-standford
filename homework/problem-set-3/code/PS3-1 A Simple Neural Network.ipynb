{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PS3-1 A Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall the following equations:\n",
    "\n",
    "\\begin{align*}\n",
    "z^{[1]} & = W^{[1]} x + W_0^{[1]} \\\\\n",
    "h & = \\sigma (z^{[1]}) \\\\\n",
    "z^{[2]} & = W^{[2]} h + W_0^{[2]} \\\\\n",
    "o & = \\sigma (z^{[2]}) \\\\\n",
    "\\ell & = \\frac{1}{m} \\sum_{i = 1}^{m} (o^{(i)} - y^{(i)})^2 = \\frac{1}{m} \\sum_{i = 1}^{m} J^{(i)}\n",
    "\\end{align*}\n",
    "\n",
    "For a single training example,\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial w_{1,2}^{[1]}} & = \\frac{\\partial J}{\\partial o} \\frac{\\partial o}{\\partial z^{[2]}} \\frac{\\partial z^{[2]}}{\\partial h_2} \\frac{\\partial h_2}{\\partial z_2^{[1]}} \\frac{\\partial z_2^{[1]}}{\\partial w_{1,2}^{[1]}} \\\\\n",
    "                                          & = 2 (o - y) \\cdot o (1 - o) \\cdot w_2^{[2]} \\cdot h_2 (1 - h_2) \\cdot x_1\n",
    "\\end{align*}\n",
    "\n",
    "where $h_2 = w_{1,2}^{[1]} x_1 + w_{2,2}^{[1]} x_2 + w_{0,2}^{[1]}$.\n",
    "\n",
    "Therefore, the gradient descent update rule for $w_{1,2}^{[1]}$ is\n",
    "\n",
    "$$w_{1,2}^{[1]} := w_{1,2}^{[1]} - \\alpha \\frac{2}{m} \\sum_{i = 1}^{m} (o^{(i)} - y^{(i)}) \\cdot o^{(i)} (1 - o^{(i)}) \\cdot w_2^{[2]} \\cdot h_2^{(i)} (1 - h_2^{(i)}) \\cdot x_1^{(i)}$$\n",
    "\n",
    "where $h_2^{(i)} = w_{1,2}^{[1]} x_1^{(i)} + w_{2,2}^{[1]} x_2^{(i)} + w_{0,2}^{[1]}$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is possible. The three neurons can be treated as three independent linear classifiers. The three decision boundaries\n",
    "form a triangle that classifies the outside data into class 1, and the inside ones into class 0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{align*}\n",
    "w_{1,1}^{[1]} x_1 + x_{2,1}^{[1]} x_2 + w_{0,1}^{[1]} & = 0 \\\\\n",
    "w_{1,2}^{[1]} x_1 + x_{2,2}^{[1]} x_2 + w_{0,2}^{[1]} & = 0 \\\\\n",
    "w_{1,3}^{[1]} x_1 + x_{2,3}^{[1]} x_2 + w_{0,3}^{[1]} & = 0\n",
    "\\end{align*}\n",
    "\n",
    "Plug in some data points and solve the equations, we can obtain the weights. The weights vary upon the choice of the decision boundaries.\n",
    "Here is one possible solution:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "w = {}\n",
    "\n",
    "w['hidden_layer_0_1'] = 0.5\n",
    "w['hidden_layer_1_1'] = -1\n",
    "w['hidden_layer_2_1'] = 0\n",
    "w['hidden_layer_0_2'] = 0.5\n",
    "w['hidden_layer_1_2'] = 0\n",
    "w['hidden_layer_2_2'] = -1\n",
    "w['hidden_layer_0_3'] = -4\n",
    "w['hidden_layer_1_3'] = 1\n",
    "w['hidden_layer_2_3'] = 1\n",
    "\n",
    "w['output_layer_0'] = -0.5\n",
    "w['output_layer_1'] = 1\n",
    "w['output_layer_2'] = 1\n",
    "w['output_layer_3'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No, it is not possible to achieve 100% accuracy using identity function as the activation functions for $h_1$, $h_2$ and $h_3$. Because\n",
    "\n",
    "\\begin{align*}\n",
    "o & = \\sigma (z^{[2]}) \\\\\n",
    "  & = \\sigma (W^{[2]} h + W_0^{[2]}) \\\\\n",
    "  & = \\sigma (W^{[2]} (W^{[1]} x + W_0^{[1]}) + W_0^{[2]}) \\\\\n",
    "  & = \\sigma (W^{[2]} W^{[1]} x + W^{[2]} W_0^{[1]} + W_0^{[2]}) \\\\\n",
    "  & = \\sigma (\\tilde{W} x + \\tilde{W_0})\n",
    "\\end{align*}\n",
    "\n",
    "where $\\tilde{W} = W^{[2]} W^{[1]}$ and $\\tilde{W_0} = W^{[2]} W_0^{[1]} + W_0^{[2]}$.\n",
    "\n",
    "We can see that the resulting classifier is still linear, and it is not able to classify datasets that are not linearly separable with 100% accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "cs229",
   "language": "python",
   "display_name": "cs229"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}